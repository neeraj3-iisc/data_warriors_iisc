{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5393f401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PySpark only - no visualization libraries\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark libraries only\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, year, month, dayofmonth, quarter, to_date\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"Using PySpark only - no visualization libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b233e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 15:05:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Spark UI: http://10.233.81.94:4040\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EV_MultiModel_Comparison\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603c799",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da03999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from HDFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 200,040 rows, 35 columns\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from HDFS\n",
    "hdfs_path = \"hdfs://localhost:9000/des/data/ev_ice_timeseries_1667_locations_120months_200krows_tweaked_for_events_and_noise.csv\"\n",
    "\n",
    "print(\"Loading data from HDFS...\")\n",
    "df = spark.read.csv(\n",
    "    hdfs_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    mode=\"PERMISSIVE\",\n",
    "    nullValue=\"NULL\",\n",
    "    nanValue=\"NaN\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded {df.count():,} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615fd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 15:05:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=====>                                                   (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Cleaned dataset: 200,040 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "print(\"\\nCleaning data...\")\n",
    "\n",
    "# Replace 'None' strings with nulls\n",
    "for column in df.columns:\n",
    "    df = df.withColumn(\n",
    "        column,\n",
    "        F.when(F.col(column).isin(['None', 'NULL', '', 'NaN']), None).otherwise(F.col(column))\n",
    "    )\n",
    "\n",
    "# Remove duplicates\n",
    "original_count = df.count()\n",
    "df = df.dropDuplicates()\n",
    "print(f\"Removed {original_count - df.count():,} duplicates\")\n",
    "\n",
    "# Impute missing numeric values with median\n",
    "numeric_cols = [field.name for field in df.schema.fields \n",
    "               if str(field.dataType) in ['IntegerType', 'DoubleType', 'FloatType', 'LongType']]\n",
    "target_col = 'ev_adoption_probability'\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    if col_name != target_col:\n",
    "        median_val = df.stat.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "        df = df.withColumn(col_name, F.when(F.col(col_name).isNull(), median_val).otherwise(F.col(col_name)))\n",
    "\n",
    "# Fill categorical with 'Unknown'\n",
    "categorical_cols = [field.name for field in df.schema.fields if str(field.dataType) == 'StringType']\n",
    "for col_name in categorical_cols:\n",
    "    df = df.withColumn(col_name, F.when(F.col(col_name).isNull(), 'Unknown').otherwise(F.col(col_name)))\n",
    "\n",
    "# Drop rows with null target\n",
    "df = df.filter(F.col(target_col).isNotNull())\n",
    "\n",
    "print(f\"âœ“ Cleaned dataset: {df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b082c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features...\n",
      "Categorical features: 0\n",
      "Numeric features: 0\n",
      "Total features: 0\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "print(\"\\nEngineering features...\")\n",
    "\n",
    "if 'date' in df.columns:\n",
    "    df = df.withColumn('date_parsed', to_date(F.col('date')))\n",
    "    df = df.withColumn('year', year('date_parsed'))\n",
    "    df = df.withColumn('month', month('date_parsed'))\n",
    "    df = df.withColumn('day', dayofmonth('date_parsed'))\n",
    "    df = df.withColumn('quarter', quarter('date_parsed'))\n",
    "\n",
    "# Define features\n",
    "exclude_cols = {target_col, 'date', 'date_parsed'}\n",
    "all_cols = set(df.columns) - exclude_cols\n",
    "\n",
    "categorical_features = []\n",
    "numeric_features = []\n",
    "\n",
    "for col_name in all_cols:\n",
    "    dtype = type(df.schema[col_name].dataType).__name__\n",
    "    if dtype == 'StringType':\n",
    "        categorical_features.append(col_name)\n",
    "    elif dtype in ['IntegerType', 'DoubleType', 'FloatType', 'LongType']:\n",
    "        numeric_features.append(col_name)\n",
    "\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Total features: {len(categorical_features) + len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191481b5",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84fc3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 160,219 rows (80.1%)\n",
      "Test set: 39,821 rows (19.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "total_count = train_count + test_count\n",
    "\n",
    "print(f\"Training set: {train_count:,} rows ({train_count/total_count:.1%})\")\n",
    "print(f\"Test set: {test_count:,} rows ({test_count/total_count:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e1b7a",
   "metadata": {},
   "source": [
    "## 3. Build Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b42873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature pipeline created with 0 input features\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing stages (shared across all models)\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") \n",
    "           for c in categorical_features]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_oh\") \n",
    "           for c in categorical_features]\n",
    "\n",
    "assembler_inputs = [f\"{c}_oh\" for c in categorical_features] + numeric_features\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "print(f\"Feature pipeline created with {len(assembler_inputs)} input features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa73ac",
   "metadata": {},
   "source": [
    "## 4. Model 1: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 1: RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "rf_pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = rf_pipeline.fit(train_df)\n",
    "print(\"âœ“ Training complete\")\n",
    "\n",
    "# Predict\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
    "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
    "rf_mae = evaluator_mae.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"  RMSE: {rf_rmse:.6f}\")\n",
    "print(f\"  RÂ²:   {rf_r2:.6f}\")\n",
    "print(f\"  MAE:  {rf_mae:.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b9bc6",
   "metadata": {},
   "source": [
    "## 5. Model 2: Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 2: GRADIENT BOOSTING REGRESSOR (GBT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create model\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "gbt_pipeline = Pipeline(stages=indexers + encoders + [assembler, gbt])\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining Gradient Boosting...\")\n",
    "gbt_model = gbt_pipeline.fit(train_df)\n",
    "print(\"âœ“ Training complete\")\n",
    "\n",
    "# Predict\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# Evaluate\n",
    "gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)\n",
    "gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
    "gbt_mae = evaluator_mae.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"\\nGradient Boosting Results:\")\n",
    "print(f\"  RMSE: {gbt_rmse:.6f}\")\n",
    "print(f\"  RÂ²:   {gbt_r2:.6f}\")\n",
    "print(f\"  MAE:  {gbt_mae:.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9df84d",
   "metadata": {},
   "source": [
    "## 6. Model 3: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7917e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 3: LINEAR REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "lr_pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining Linear Regression...\")\n",
    "lr_model = lr_pipeline.fit(train_df)\n",
    "print(\"âœ“ Training complete\")\n",
    "\n",
    "# Predict\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
    "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"\\nLinear Regression Results:\")\n",
    "print(f\"  RMSE: {lr_rmse:.6f}\")\n",
    "print(f\"  RÂ²:   {lr_r2:.6f}\")\n",
    "print(f\"  MAE:  {lr_mae:.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05d808",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = ['Random Forest', 'Gradient Boosting', 'Linear Regression']\n",
    "rmse_vals = [rf_rmse, gbt_rmse, lr_rmse]\n",
    "r2_vals = [rf_r2, gbt_r2, lr_r2]\n",
    "mae_vals = [rf_mae, gbt_mae, lr_mae]\n",
    "\n",
    "print(f\"\\n{'Model':<20} {'RMSE':<15} {'RÂ²':<15} {'MAE':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for i in range(len(models)):\n",
    "    print(f\"{models[i]:<20} {rmse_vals[i]:<15.6f} {r2_vals[i]:<15.6f} {mae_vals[i]:<15.6f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e561bf9",
   "metadata": {},
   "source": [
    "## 8. Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model\n",
    "best_idx = r2_vals.index(max(r2_vals))\n",
    "best_model_name = models[best_idx]\n",
    "best_r2 = r2_vals[best_idx]\n",
    "best_rmse = rmse_vals[best_idx]\n",
    "best_mae = mae_vals[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ† BEST MODEL SELECTED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {best_model_name}\")\n",
    "print(f\"RÂ² Score: {best_r2:.6f}\")\n",
    "print(f\"RMSE: {best_rmse:.6f}\")\n",
    "print(f\"MAE: {best_mae:.6f}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Set best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    print(\"\\nâœ“ Random Forest provides the best balance of accuracy and interpretability.\")\n",
    "    best_model = rf_model\n",
    "    best_predictions = rf_predictions\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    print(\"\\nâœ“ Gradient Boosting achieves the highest accuracy.\")\n",
    "    best_model = gbt_model\n",
    "    best_predictions = gbt_predictions\n",
    "else:\n",
    "    print(\"\\nâœ“ Linear Regression is the simplest model.\")\n",
    "    best_model = lr_model\n",
    "    best_predictions = lr_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf04e6d",
   "metadata": {},
   "source": [
    "## 9. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "print(f\"\\nSample predictions from {best_model_name}:\")\n",
    "best_predictions.select(target_col, 'prediction').show(20)\n",
    "\n",
    "# Error analysis\n",
    "predictions_with_error = best_predictions.withColumn(\n",
    "    'error', \n",
    "    F.col('prediction') - F.col(target_col)\n",
    ").withColumn(\n",
    "    'abs_error',\n",
    "    F.abs(F.col('error'))\n",
    ")\n",
    "\n",
    "print(f\"\\nError statistics:\")\n",
    "predictions_with_error.select('error', 'abs_error').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7c133",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "from pyspark.sql import Row\n",
    "\n",
    "results_data = [\n",
    "    Row(Model=models[i], RMSE=rmse_vals[i], R2=r2_vals[i], MAE=mae_vals[i])\n",
    "    for i in range(len(models))\n",
    "]\n",
    "results_df = spark.createDataFrame(results_data)\n",
    "\n",
    "results_df.coalesce(1).write.csv('outputs/model_comparison_results', header=True, mode='overwrite')\n",
    "print(\"âœ“ Model comparison results saved to outputs/model_comparison_results/\")\n",
    "\n",
    "# Save best model predictions\n",
    "best_predictions.select('location', 'date', target_col, 'prediction') \\\n",
    "    .coalesce(1) \\\n",
    "    .write.csv('outputs/best_model_predictions', header=True, mode='overwrite')\n",
    "print(f\"âœ“ Best model ({best_model_name}) predictions saved to outputs/best_model_predictions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dceb5e",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b59333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset: {total_count:,} rows ({train_count:,} train, {test_count:,} test)\")\n",
    "print(f\"Features: {len(categorical_features) + len(numeric_features)} ({len(categorical_features)} categorical, {len(numeric_features)} numeric)\")\n",
    "print(f\"\\nModels trained: {len(models)}\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Best RÂ²: {best_r2:.6f}\")\n",
    "print(f\"Best RMSE: {best_rmse:.6f}\")\n",
    "print(f\"Best MAE: {best_mae:.6f}\")\n",
    "print(\"\\nâœ“ Multi-model training and comparison complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9829be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"âœ“ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
